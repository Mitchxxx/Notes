---------CHEF-------

uninstall chef
dpkg --list | grep chef # or dpkg --status chef
then purge (see man dpkg):
dpkg -P chef

sudo apt-get remove apache2
sudo apt-get purge apache2
sudo apt-get autoremove

echo "cookbook_path [ '/home/imran/TestDevProjects/chef-repo/cookbooks' ]" > .chef/knife.rb

Connect (SSH) to aws vm
node1

bootstrap a node (aws vm) to Chef server - chef adds the client.pem to the client

with ssh key
knife bootstrap ADDRESS --ssh-user USER --sudo --identity-file IDENTITY_FILE --node-name node1-ubuntu --run-list 'recipe[learn_chef_apache2]'
knife bootstrap 54.165.69.191 --ssh-user ec2-user -i aws.pem --sudo --node-name node1 --run-list 'recipe[learn_chef_apache2]'

with password
knife bootstrap 192.168.0.15 --ssh-user imran --ssh-password 'orange' --sudo --use-sudo-password --node-name node1-ubuntu --run-list 'recipe[learn_chef_apache2]'



when bootstrapping ensure /etc/chef/client.pem is removed from the node.

Attach a recipe to node
knife node run_list set test-node 'recipe[iptables]'


knife cookbook create COOKBOOK_NAME 
chef generate cookbook learn_chef_apache2
knife node show nodename
knife download cookbooks 'name'
knife cookbook delete cookbook_name
knife cookbook upload cookbook_name
knife node list

knife node run_list add NODE_NAME RUN_LIST_ITEM (options)
knife node run_list set node2 recipe[java]
knife node run_list remove NODENAME role[myrole],recipe[myrecipe]

edit data of an node
knife node edit node1 -a
Details about the node
knife role show node1

Install cookbook to node (run on node)
chef-client



*************************************HADOOP********************************************************

FROM sequenceiq/pam:ubuntu-14.04
MAINTAINER lewuathe

ENV HADOOP_VERSION 3.0.0-alpha4
ENV HADOOP_HOME /usr/local/hadoop
ENV JAVA_VERSION 1.8.0_144

# install dev tools
RUN apt-get update
RUN apt-get install -y wget curl tar sudo openssh-server openssh-client rsync

# passwordless ssh
RUN rm -f /etc/ssh/ssh_host_dsa_key /etc/ssh/ssh_host_rsa_key /root/.ssh/id_rsa
RUN ssh-keygen -q -N "" -t dsa -f /etc/ssh/ssh_host_dsa_key
RUN ssh-keygen -q -N "" -t rsa -f /etc/ssh/ssh_host_rsa_key
RUN ssh-keygen -q -N "" -t rsa -f /root/.ssh/id_rsa
RUN cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys


# java
RUN cd /usr/local && \
   curl -LkO --header "Cookie: oraclelicense=accept-securebackup-cookie" "http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz" && \
   tar -xzvf jdk-8u144-linux-x64.tar.gz && \
   rm jdk-8u144-linux-x64.tar.gz

ENV JAVA_HOME /usr/local/jdk$JAVA_VERSION
ENV PATH $PATH:$JAVA_HOME/bin

# HADOOP
WORKDIR /usr/local

RUN cd /usr/local && \
    wget http://apache.claz.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz && \
    tar -xzvf hadoop-$HADOOP_VERSION.tar.gz && \
    mv hadoop-$HADOOP_VERSION hadoop && \
    rm hadoop-$HADOOP_VERSION.tar.gz

ENV HADOOP_PREFIX /usr/local/hadoop
ENV HADOOP_COMMON_HOME /usr/local/hadoop
ENV HADOOP_HDFS_HOME /usr/local/hadoop
ENV HADOOP_MAPRED_HOME /usr/local/hadoop
ENV HADOOP_YARN_HOME /usr/local/hadoop
ENV HADOOP_CONF_DIR /usr/local/hadoop/etc/hadoop
ENV YARN_CONF_DIR $HADOOP_PREFIX/etc/hadoop
ENV HADOOP_COMMON_LIB_NATIVE_DIR /usr/local/hadoop/lib/native export
ENV PATH $PATH:$/usr/local/hadoop/sbin:/usr/local/hadoop/bin

RUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/java/default\nexport HADOOP_PREFIX=/usr/local/hadoop\nexport HADOOP_HOME=/usr/local/hadoop\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh
RUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh
#RUN . $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh

RUN mkdir $HADOOP_PREFIX/input
RUN cp $HADOOP_PREFIX/etc/hadoop/*.xml $HADOOP_PREFIX/input

# pseudo distributed
ADD core-site.xml.template $HADOOP_PREFIX/etc/hadoop/core-site.xml.template
RUN sed s/HOSTNAME/localhost/ /usr/local/hadoop/etc/hadoop/core-site.xml.template > /usr/local/hadoop/etc/hadoop/core-site.xml
ADD hdfs-site.xml $HADOOP_PREFIX/etc/hadoop/hdfs-site.xml

ADD mapred-site.xml $HADOOP_PREFIX/etc/hadoop/mapred-site.xml
ADD yarn-site.xml $HADOOP_PREFIX/etc/hadoop/yarn-site.xml

RUN $HADOOP_PREFIX/bin/hdfs namenode -format

# fixing the libhadoop.so like a boss
RUN rm -rf /usr/local/hadoop/lib/native
RUN mv /tmp/native /usr/local/hadoop/lib

ADD ssh_config /root/.ssh/config
RUN chmod 600 /root/.ssh/config
RUN chown root:root /root/.ssh/config

ADD bootstrap.sh /etc/bootstrap.sh
RUN chown root:root /etc/bootstrap.sh
RUN chmod 700 /etc/bootstrap.sh

ENV BOOTSTRAP /etc/bootstrap.sh

# workingaround docker.io build error
RUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh
RUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh
RUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh

# fix the 254 error code
RUN sed  -i "/^[^#]*UsePAM/ s/.*/#&/"  /etc/ssh/sshd_config
RUN echo "UsePAM no" >> /etc/ssh/sshd_config
RUN echo "Port 2122" >> /etc/ssh/sshd_config

RUN service sshd start && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh && $HADOOP_PREFIX/sbin/start-dfs.sh && $HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/root
RUN service sshd start && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh && $HADOOP_PREFIX/sbin/start-dfs.sh && $HADOOP_PREFIX/bin/hdfs dfs -put $HADOOP_PREFIX/etc/hadoop/ input

# Hdfs ports
EXPOSE 50010 50020 50070 50075 50090 8020 9000
# Mapred ports
EXPOSE 10020 19888
#Yarn ports
EXPOSE 8030 8031 8032 8033 8040 8042 8088
#Other ports
EXPOSE 49707 2122

ADD bootstrap.sh /etc/bootstrap.sh
RUN chown root:root /etc/bootstrap.sh
RUN chmod 700 /etc/bootstrap.sh

#ENV BOOTSTRAP /etc/bootstrap.sh

CMD ["/etc/bootstrap.sh", "-d"]

********************************HIVE*****************************


FROM lewuathe/hadoop-base:latest
MAINTAINER Imran Awan

ENV HIVE_VERSION 2.3.0
ENV HIVE_HOME /usr/local/hive
#ENV PATH $HIVE_HOME/bin:$PATH
ENV HADOOP_HOME /usr/local/hadoop
ENV BOOTSTRAP /etc/bootstrap.sh
ENV DERBY_VERSION 10.9.1.0
ENV DERBY_HOME /usr/local/derby

# Copy the hadoop config files
COPY conf-hadoop/core-site.xml $HADOOP_HOME/etc/hadoop
COPY conf-hadoop/hdfs-site.xml $HADOOP_HOME/etc/hadoop
COPY conf-hadoop/mapred-site.xml $HADOOP_HOME/etc/hadoop
COPY conf-hadoop/yarn-site.xml $HADOOP_HOME/etc/hadoop


# Create HDFS / Hive directories
RUN mkdir -p /home/hadoop/hadoopinfra/hdfs/namenode && \
    mkdir /home/hadoop/hadoopinfra/hdfs/datanode && \
    mkdir -p /user/hive/warehouse && \
        mkdir -p $DERBY_HOME/data
    # Create a directory to store Metastore


WORKDIR /usr/local

# *****Download Hive********


RUN apt-get update && apt-get install -y wget procps && \
    wget http://archive.apache.org/dist/hive/hive-$HIVE_VERSION/apache-hive-$HIVE_VERSION-bin.tar.gz && \
    tar -xzvf apache-hive-$HIVE_VERSION-bin.tar.gz && \
    mv apache-hive-$HIVE_VERSION-bin $HIVE_HOME && \
    wget https://jdbc.postgresql.org/download/postgresql-9.4.1212.jar -O $HIVE_HOME/lib/postgresql-jdbc.jar && \
    rm apache-hive-$HIVE_VERSION-bin.tar.gz && \
    #apt-get --purge remove -y wget && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

        # Copy the hive config files
COPY conf-hive/hive-site.xml $HIVE_HOME/conf
COPY conf-hive/jpox.properties $HIVE_HOME/conf


# edit profile environment
RUN echo "export HADOOP_HOME=/usr/local/hadoop" >> ~/.bashrc && \
	echo "export HADOOP_MAPRED_HOME=$HADOOP_HOME" >> ~/.bashrc && \
	echo "export HADOOP_COMMON_HOME=$HADOOP_HOME" >> ~/.bashrc && \
	echo "export HADOOP_HDFS_HOME=$HADOOP_HOME" >> ~/.bashrc && \
	echo "export YARN_HOME=$HADOOP_HOME" >> ~/.bashrc && \
	echo "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export" >> ~/.bashrc && \
	echo "export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin" >> ~/.bashrc && \
    # hive profile
    echo "export HIVE_HOME=$HIVE_HOME" >> ~/.bashrc && \
    echo "export PATH=$PATH:$HIVE_HOME/bin" >> ~/.bashrc && \
    echo "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native" >> ~/.bashrc && \
    echo "export HADOOP_OPTS=-Djava.library.path=$HADOOP_HOME/lib" >> ~/.bashrc && \
    echo "export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:." >> ~/.bashrc && \
    echo "export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib/*:." >> ~/.bashrc

CMD source ~/.bashrc

ENV HADOOP_HOME /usr/local/hadoop
ENV HADOOP_MAPRED_HOME $HADOOP_HOME
ENV HADOOP_COMMON_HOME $HADOOP_HOME
ENV HADOOP_HDFS_HOME $HADOOP_HOME
ENV YARN_HOME $HADOOP_HOME
ENV HADOOP_COMMON_LIB_NATIVE_DIR $HADOOP_HOME/lib/native export
ENV PATH $PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin



#**************************

# Format HDFS
#RUN $HADOOP_HOME/bin/hdfs namenode -format

RUN $HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/hive/warehouse; exit 0 && \
    $HADOOP_HOME/bin/hdfs dfs -mkdir /tmp; exit 0 && \
#Set read/write permissions for table.
    $HADOOP_HOME/bin/hdfs dfs -chmod g+w /user/hive/warehouse; exit 0 && \
    $HADOOP_HOME/bin/hdfs dfs -chmod g+w /tmp; exit 0

RUN echo "export HADOOP_HOME=$HADOOP_HOME" >> $HIVE_HOME/conf/hive-env.sh && \
echo "export HADOOP_HEAPSIZE=512" >> $HIVE_HOME/conf/hive-env.sh && \
echo "export HIVE_CONF_DIR=$HIVE_HOME/conf" >> $HIVE_HOME/conf/hive-env.sh

#***********Download Derby************************

# By default, Hive uses Derby database. Initialize Derby database.

RUN wget http://archive.apache.org/dist/db/derby/db-derby-$DERBY_VERSION/db-derby-$DERBY_VERSION-bin.tar.gz && \
    tar -xzvf db-derby-$DERBY_VERSION-bin.tar.gz && \
    #mkdir /usr/local/derby && \
    mv db-derby-$DERBY_VERSION-bin $DERBY_HOME && \
    rm db-derby-$DERBY_VERSION-bin.tar.gz && \
    apt-get --purge remove -y wget && \
    apt-get clean

    # edit profile environment
RUN echo "export DERBY_HOME=$DERBY_HOME" >> ~/.bashrc && \
    echo "export PATH=$PATH:$DERBY_HOME/bin" >> ~/.bashrc && \
    echo "export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derby.jar:$DERBY_HOME/lib/derbytools.jar" >> ~/.bashrc

CMD source ~/.bashrc

ADD bootstrap.sh /etc/bootstrap.sh
RUN chown root:root /etc/bootstrap.sh
RUN chmod 700 /etc/bootstrap.sh

ENV BOOTSTRAP /etc/bootstrap.sh

CMD ["/etc/bootstrap.sh", "-d"]




docker rmi -f 759c1a075775
docker rmi -f 662b276ddcbf
docker rmi -f 5409820111a1
docker rmi -f cd83a6a7312e
docker rmi -f 657706cda170L


